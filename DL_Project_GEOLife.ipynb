{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load Modules\n",
        "The google drive path with the data is here:\n",
        "https://drive.google.com/drive/folders/1SvfyjqJLUp6ma2q-xGjmC1gqt1fzhNGW?usp=sharing"
      ],
      "metadata": {
        "id": "5Du2gp4CsELq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/Colab Notebooks')\n",
        "!pip3 install -r /content/gdrive/My\\ Drive/Colab\\ Notebooks/GLC/requirements.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSgJMeRqLzwR",
        "outputId": "8135d846-27c5-4ddc-cce2-f75b9f48c23a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n",
            "Requirement already satisfied: cartopy in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 1)) (0.19.0.post1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 2)) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 3)) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 4)) (1.3.5)\n",
            "Requirement already satisfied: rasterio==1.0.24 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 5)) (1.0.24)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 6)) (1.11.0+cu113)\n",
            "Requirement already satisfied: affine in /usr/local/lib/python3.7/dist-packages (from rasterio==1.0.24->-r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 5)) (2.3.1)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.7/dist-packages (from rasterio==1.0.24->-r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 5)) (1.1.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.7/dist-packages (from rasterio==1.0.24->-r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 5)) (0.7.2)\n",
            "Requirement already satisfied: snuggs>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from rasterio==1.0.24->-r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 5)) (1.4.7)\n",
            "Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.7/dist-packages (from rasterio==1.0.24->-r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from rasterio==1.0.24->-r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 5)) (21.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.7/dist-packages (from snuggs>=1.4.1->rasterio==1.0.24->-r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 5)) (3.0.9)\n",
            "Requirement already satisfied: pyshp>=2 in /usr/local/lib/python3.7/dist-packages (from cartopy->-r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 1)) (2.3.0)\n",
            "Requirement already satisfied: shapely>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from cartopy->-r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 1)) (1.8.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 2)) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 2)) (1.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->-r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 2)) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->-r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 4)) (2022.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Furvgx9oIPHD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
        "import copy\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from GLC.data_loading.environmental_raster import PatchExtractor\n",
        "from GLC.data_loading.common import load_patch\n",
        "from GLC.metrics import predict_top_30_set\n",
        "from GLC.submission import generate_submission_file\n",
        "from GLC.metrics import top_k_error_rate_from_sets\n",
        "from GLC.metrics import top_30_error_rate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "  tf.config.experimental_connect_to_cluster(resolver)\n",
        "  tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "  print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "  strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "except ValueError:\n",
        "  strategy = tf.distribute.get_strategy() "
      ],
      "metadata": {
        "id": "VI2l8rz4w488"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Training Data**\n"
      ],
      "metadata": {
        "id": "kesR5k7wgqhi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load Complete Dataset**\n",
        "\n",
        "We can use the whole dataset to train on the environmental vectors since it is not too computationaly expensive to do so"
      ],
      "metadata": {
        "id": "WtgcgG_YhIoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# SUBMISSION_PATH = Path(\"submissions\")\n",
        "# os.makedirs(SUBMISSION_PATH, exist_ok=True)\n",
        "\n",
        "DATA_PATH = Path(\"/content/gdrive/My Drive/Colab Notebooks/input/\")\n",
        "\n",
        "df_obs_fr = pd.read_csv(DATA_PATH / \"observations\" / \"observations_fr_train.csv\", sep=\";\", index_col=\"observation_id\")\n",
        "df_obs_us = pd.read_csv(DATA_PATH / \"observations\" / \"observations_us_train.csv\", sep=\";\", index_col=\"observation_id\")\n",
        "df_obs = pd.concat((df_obs_fr, df_obs_us))\n",
        "\n",
        "obs_id_train = df_obs.index[df_obs[\"subset\"] == \"train\"].values\n",
        "obs_id_val = df_obs.index[df_obs[\"subset\"] == \"val\"].values\n",
        "\n",
        "y_train = df_obs.loc[obs_id_train][\"species_id\"].values\n",
        "y_val = df_obs.loc[obs_id_val][\"species_id\"].values\n",
        "\n",
        "n_val = len(obs_id_val)\n",
        "print(\"Validation set size: {} ({:.1%} of train observations)\".format(n_val, n_val / len(df_obs)))\n",
        "\n",
        "# df_obs_fr_test = pd.read_csv(DATA_PATH / \"observations\" / \"observations_fr_test.csv\", sep=\";\", index_col=\"observation_id\")\n",
        "# df_obs_us_test = pd.read_csv(DATA_PATH / \"observations\" / \"observations_us_test.csv\", sep=\";\", index_col=\"observation_id\")\n",
        "# df_obs_test = pd.concat((df_obs_fr_test, df_obs_us_test))\n",
        "# obs_id_test = df_obs_test.index.values\n",
        "# print(\"Number of observations for testing: {}\".format(len(df_obs_test)))\n",
        "# print(df_obs_test.head())\n",
        "\n",
        "df_env = pd.read_csv(DATA_PATH / \"pre-extracted\" / \"environmental_vectors.csv\", sep=\";\", index_col=\"observation_id\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZKoPWp5glr5",
        "outputId": "50608c2b-b6d9-437c-fd20-90166a2bd11b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation set size: 40080 (2.5% of train observations)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Load a subset of the data**\n",
        "\n",
        "We can use a subset of the dataset to train on the CNNs and Inception v2 since it is too computationaly expensive to do so.*italicised text*\n"
      ],
      "metadata": {
        "id": "ErEHgLsOhYVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_observations = 100000\n",
        "n_species = 500\n",
        "\n",
        "path_temp = \"data\" + str(n_observations) + \"-\" + str(500) + \".csv\"\n",
        "df_obs_subset = pd.read_csv(DATA_PATH / \"data-subset\" / path_temp, sep=\",\", index_col=\"observation_id\")\n",
        "print(\"Number of observations for training: {}\".format(len(df_obs_subset)))\n",
        "y_true = df_obs_subset['species_id']\n",
        "# Relabel\n",
        "\n",
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(df_obs_subset.species_id)\n",
        "df_obs_subset['species_id'] = le.transform(df_obs_subset.species_id)\n",
        "number_of_unique_species = np.unique(df_obs_subset['species_id']).shape[0]\n",
        "print(\"Number of unique species: \"+str(number_of_unique_species))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAvlxZpsgnin",
        "outputId": "8eda3e08-33a1-4ed8-82c5-8ce2e90d70ed"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of observations for training: 100000\n",
            "Number of unique species: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest on Environmental Vectors"
      ],
      "metadata": {
        "id": "pgCsTzsMierU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = df_env.loc[obs_id_train].values\n",
        "\n",
        "X_val = df_env.loc[obs_id_val].values\n",
        "# X_test = df_env.loc[obs_id_test].values\n",
        "\n",
        "# print(y_train)\n",
        "imp = SimpleImputer(\n",
        "    missing_values=np.nan,\n",
        "    strategy=\"constant\",\n",
        "    fill_value=np.finfo(np.float32).min,\n",
        ")\n",
        "imp.fit(X_train)\n",
        "\n",
        "X_train = imp.transform(X_train)\n",
        "X_val = imp.transform(X_val)\n",
        "# X_test = imp.transform(X_test)\n",
        "n_features = X_train.shape[1]\n",
        "# print(X_train)\n",
        "# print(\"Rescaling\")\n",
        "#X_train = X_train / X_train.max(axis=0)\n",
        "#X_val = X_val / X_val.max(axis=0)\n",
        "#X_test = X_test / X_test.max(axis=0)\n",
        "\n",
        "\"\"\"\n",
        "model = Sequential()\n",
        "model.add(Dense(32, activation='relu',\n",
        "                kernel_initializer='he_normal', input_shape=(n_features,)))\n",
        "model.add(Dense(16, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(1, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, epochs=1, batch_size=32, verbose=1)\n",
        "\n",
        "\"\"\"\n",
        "filename = \"/content/gdrive/My Drive/Colab Notebooks/rf.sav\"\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=64, max_depth=10,  verbose=0, n_jobs=-1)\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "pickle.dump(est, open(filename, 'wb'))\n",
        "\n",
        "# rf_model = pickle.load(open(filename, 'rb'))\n",
        "\n",
        "def batch_predict(predict_func, X, batch_size=1024):\n",
        "    res = predict_func(X[:1])\n",
        "    n_samples, n_outputs, dtype = X.shape[0], res.shape[1], res.dtype\n",
        "\n",
        "    preds = np.empty((n_samples, n_outputs), dtype=dtype)\n",
        "\n",
        "    for i in range(0, len(X), batch_size):\n",
        "        X_batch = X[i:i+batch_size]\n",
        "        preds[i:i+batch_size] = predict_func(X_batch)\n",
        "\n",
        "    return preds\n",
        "\n",
        "\n",
        "def predict_func(X):\n",
        "    y_score = rf_model.predict_proba(X)\n",
        "    s_pred = predict_top_30_set(y_score)\n",
        "    return s_pred\n",
        "\n",
        "\n",
        "s_val = batch_predict(predict_func, X_val, batch_size=1024)\n",
        "score_val = top_k_error_rate_from_sets(y_val, s_val)\n",
        "\n",
        "print(\"Top-30 error rate: {:.1%}\".format(score_val))\n",
        "\n",
        "# s_pred = batch_predict(predict_func, X_test, batch_size=1024)\n",
        "# print(\"Generate the submission file\")\n",
        "# generate_submission_file(SUBMISSION_PATH / \"random_forest_on_environmental_vectors.csv\", df_obs_test.index, s_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "Y98NKRm4iXlC",
        "outputId": "012ccc56-535e-437a-c7ab-200e489c4fbc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "EOFError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-dba2bbdeb721>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mrf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# rf_model.fit(X_train, y_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mrf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbatch_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Neural Network (Inception v2) on image patches"
      ],
      "metadata": {
        "id": "cL6TYM6ujMn2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pvQvPcPZIPHJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5c1fcc0-0b03-4f21-c4d6-4a292ca91e39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 256, 256, 3)       165       \n",
            "                                                                 \n",
            " inception_v3 (Functional)   (None, 2048)              21802784  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 500)               1024500   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22,827,449\n",
            "Trainable params: 22,793,017\n",
            "Non-trainable params: 34,432\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications import imagenet_utils\n",
        "from keras.preprocessing.image import img_to_array, load_img\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "inception_model = Sequential()\n",
        "inception_model.add(Conv2D(3, kernel_size = 3, padding=\"same\", input_shape = (256, 256, 6), activation = 'relu'))\n",
        "inception_model.add(InceptionV3(weights='imagenet', include_top = False, input_shape= (256, 256, 3), pooling=\"avg\", classes=number_of_unique_species))\n",
        "inception_model.add(Dense(number_of_unique_species, activation = 'softmax'))\n",
        "inception_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "inception_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plotdata = []\n",
        "from sklearn.model_selection import train_test_split\n",
        "y_all = np.array(pd.get_dummies(df_obs['species_id']).values)\n",
        "start = 0\n",
        "end = 1\n",
        "for i in range(start, end):\n",
        "  path_temp = \"patches\" + str(n_observations) + \"-\" + str(n_species) + '-' + str(i) + \".npz\"\n",
        "  patches = np.load(DATA_PATH  / \"data-subset\" / \"patches\" / path_temp)['arr_0']\n",
        "  X = np.array(patches)\n",
        "  y = y_all[i*1000:(i+1)*1000]\n",
        "  X_train, X_test_and_val, y_train, y_test_and_val = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "  X_test, X_val, y_test, y_val = train_test_split(X_test_and_val, y_test_and_val, test_size=0.5, random_state=42)\n",
        "\n",
        "  print(\"\\n\\nTraining observations \" + str(i*1000+1) + \" to \" + str((i+1)*1000))\n",
        "  inception_model.fit(X_train, y_train, epochs = 50, validation_data=(X_test, y_test), batch_size = 32, verbose=1)\n",
        "  # inception_model.save_weights('/content/gdrive/My Drive/Colab Notebooks/checkpoint500-100000-'+str(i)+'.h5')\n",
        "  (loss, accuracy) = inception_model.evaluate(x = X_test, y = y_test)\n",
        "  print('Loss: {} Accuracy: {}'.format(loss, accuracy * 100))\n",
        "  predictions = inception_model.predict(X_val)\n",
        "  wrong = 0\n",
        "  for index in range(len(y_val)):\n",
        "      top_30_preds = predictions[index].argsort()[-30:][::-1]\n",
        "      if le.transform(np.where(y_val[index])[0])[0] in top_30_preds:\n",
        "        continue\n",
        "      wrong+=1\n",
        "  print(\"Top 30 Error Rate with \"+str((i+1)*1000)+\" observations used for training: \"+str(1.0*wrong/len(y_test)))\n",
        "  plotdata.append([(i+1)*1000, 1.0*wrong/len(y_test)])\n",
        "\n",
        "inception_model.save_weights('/content/gdrive/My Drive/Colab Notebooks/checkpoint500-100000.h5')\n",
        "np.savetxt('/content/gdrive/My Drive/Colab Notebooks/plotdata_'+ str(n_species) + '_'  + str(start) + '-'+str(end) + '.csv', np.array(plotdata), delimiter=',')"
      ],
      "metadata": {
        "id": "EX9Ovu2GMzfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Learning using RL and DNN"
      ],
      "metadata": {
        "id": "xZrInPT4p-pA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models_list = [rf_model, ] \n",
        "\n",
        "def stacked_dataset(members, inputX):\n",
        "\tstackX = None\n",
        "\tfor model in members:\n",
        "\t\t# make prediction\n",
        "\t\tyhat = model.predict(inputX, verbose=0)\n",
        "\t\t# stack predictions into [rows, members, probabilities]\n",
        "\t\tif stackX is None:\n",
        "\t\t\tstackX = yhat #\n",
        "\t\telse:\n",
        "\t\t\tstackX = np.dstack((stackX, yhat))\n",
        "\t# flatten predictions to [rows, members x probabilities]\n",
        "\tstackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2]))\n",
        "\treturn stackX\n",
        "\n",
        "# fit a model based on the outputs from the ensemble members\n",
        "def fit_stacked_model(members, inputX, inputy):\n",
        "\t# create dataset using ensemble\n",
        "\tstackedX = stacked_dataset(members, inputX)\n",
        "\t# fit the meta learner\n",
        "\tmodel = LogisticRegression() #meta learner\n",
        "\tmodel.fit(stackedX, inputy)\n",
        "\treturn model\n",
        "model = fit_stacked_model(members, X_test,y_test)\n",
        "\n",
        "# make a prediction with the stacked model\n",
        "def stacked_prediction(members, model, inputX):\n",
        "\t# create dataset using ensemble\n",
        "\tstackedX = stacked_dataset(members, inputX)\n",
        "\t# make a prediction\n",
        "\tyhat = model.predict(stackedX)\n",
        "\treturn yhat\n",
        "\n",
        "yhat = stacked_prediction(members, model, X_test)\n",
        "score = f1_m(y_test/1.0, yhat/1.0)\n",
        "print('Stacked F Score:', score)"
      ],
      "metadata": {
        "id": "Qy88DZBxqJCs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "colab": {
      "name": "DL Project - GEOLife.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}