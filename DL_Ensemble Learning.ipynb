{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Du2gp4CsELq"
      },
      "source": [
        "# Load Modules\n",
        "The google drive path with the data is here:\n",
        "https://drive.google.com/drive/folders/1SvfyjqJLUp6ma2q-xGjmC1gqt1fzhNGW?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSgJMeRqLzwR",
        "outputId": "450eb705-2c26-4978-de1a-1f5a8e6e1d00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n",
            "Requirement already satisfied: cartopy in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 1)) (0.19.0.post1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 2)) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 3)) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 4)) (1.3.5)\n",
            "Requirement already satisfied: rasterio==1.0.24 in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 5)) (1.0.24)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 6)) (1.11.0+cu113)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from rasterio==1.0.24->-r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 5)) (21.4.0)\n",
            "Requirement already satisfied: affine in /usr/local/lib/python3.7/dist-packages (from rasterio==1.0.24->-r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 5)) (2.3.1)\n",
            "Requirement already satisfied: click-plugins in /usr/local/lib/python3.7/dist-packages (from rasterio==1.0.24->-r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 5)) (1.1.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.7/dist-packages (from rasterio==1.0.24->-r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 5)) (0.7.2)\n",
            "Requirement already satisfied: snuggs>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from rasterio==1.0.24->-r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 5)) (1.4.7)\n",
            "Requirement already satisfied: click<8,>=4.0 in /usr/local/lib/python3.7/dist-packages (from rasterio==1.0.24->-r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 5)) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.1.6 in /usr/local/lib/python3.7/dist-packages (from snuggs>=1.4.1->rasterio==1.0.24->-r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 5)) (3.0.9)\n",
            "Requirement already satisfied: pyshp>=2 in /usr/local/lib/python3.7/dist-packages (from cartopy->-r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 1)) (2.3.0)\n",
            "Requirement already satisfied: shapely>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from cartopy->-r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 1)) (1.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 2)) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 2)) (1.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->-r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 2)) (4.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->-r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 2)) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r /content/gdrive/My Drive/Colab Notebooks/GLC/requirements.txt (line 4)) (2022.1)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/Colab Notebooks')\n",
        "!pip3 install -r /content/gdrive/My\\ Drive/Colab\\ Notebooks/GLC/requirements.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Furvgx9oIPHD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
        "import copy\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications import imagenet_utils\n",
        "from keras.preprocessing.image import img_to_array, load_img\n",
        "from keras.applications.inception_v3 import preprocess_input\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from GLC.data_loading.environmental_raster import PatchExtractor\n",
        "from GLC.data_loading.common import load_patch\n",
        "from GLC.metrics import predict_top_30_set\n",
        "from GLC.submission import generate_submission_file\n",
        "from GLC.metrics import top_k_error_rate_from_sets\n",
        "from GLC.metrics import top_30_error_rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VI2l8rz4w488"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "  tf.config.experimental_connect_to_cluster(resolver)\n",
        "  tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "  print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "  strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "except ValueError:\n",
        "  strategy = tf.distribute.get_strategy() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kesR5k7wgqhi"
      },
      "source": [
        "# **Load Training Data**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtgcgG_YhIoo"
      },
      "source": [
        "### **Load Complete Dataset**\n",
        "\n",
        "We can use the whole dataset to train on the environmental vectors since it is not too computationaly expensive to do so"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AZKoPWp5glr5",
        "outputId": "8f1d6e86-ee15-4ded-a43b-da58da24092f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation set size: 40080 (2.5% of train observations)\n"
          ]
        }
      ],
      "source": [
        "# SUBMISSION_PATH = Path(\"submissions\")\n",
        "# os.makedirs(SUBMISSION_PATH, exist_ok=True)\n",
        "\n",
        "DATA_PATH = Path(\"/content/gdrive/My Drive/Colab Notebooks/input/\")\n",
        "\n",
        "df_obs_fr = pd.read_csv(DATA_PATH / \"observations\" / \"observations_fr_train.csv\", sep=\";\", index_col=\"observation_id\")\n",
        "df_obs_us = pd.read_csv(DATA_PATH / \"observations\" / \"observations_us_train.csv\", sep=\";\", index_col=\"observation_id\")\n",
        "# df_obs_fr_test = pd.read_csv(DATA_PATH / \"observations\" / \"observations_fr_test.csv\", sep=\";\", index_col=\"observation_id\")\n",
        "# df_obs_us_test = pd.read_csv(DATA_PATH / \"observations\" / \"observations_us_test.csv\", sep=\";\", index_col=\"observation_id\")\n",
        "df_env = pd.read_csv(DATA_PATH / \"pre-extracted\" / \"environmental_vectors.csv\", sep=\";\", index_col=\"observation_id\")\n",
        "df_obs = pd.concat((df_obs_fr, df_obs_us))\n",
        "\n",
        "obs_id_train = df_obs.index[df_obs[\"subset\"] == \"train\"].values\n",
        "obs_id_val = df_obs.index[df_obs[\"subset\"] == \"val\"].values\n",
        "\n",
        "y_train = df_obs.loc[obs_id_train][\"species_id\"].values\n",
        "y_val = df_obs.loc[obs_id_val][\"species_id\"].values\n",
        "\n",
        "n_val = len(obs_id_val)\n",
        "print(\"Validation set size: {} ({:.1%} of train observations)\".format(n_val, n_val / len(df_obs)))\n",
        "\n",
        "X_train = df_env.loc[obs_id_train].values\n",
        "\n",
        "X_val = df_env.loc[obs_id_val].values\n",
        "# X_test = df_env.loc[obs_id_test].values\n",
        "\n",
        "# print(y_train)\n",
        "imp = SimpleImputer(\n",
        "    missing_values=np.nan,\n",
        "    strategy=\"constant\",\n",
        "    fill_value=np.finfo(np.float32).min,\n",
        ")\n",
        "imp.fit(X_train)\n",
        "\n",
        "X_train = imp.transform(X_train)\n",
        "X_val = imp.transform(X_val)\n",
        "# X_test = imp.transform(X_test)\n",
        "n_features = X_train.shape[1]\n",
        "\n",
        "# df_obs_test = pd.concat((df_obs_fr_test, df_obs_us_test))\n",
        "# obs_id_test = df_obs_test.index.values\n",
        "# print(\"Number of observations for testing: {}\".format(len(df_obs_test)))\n",
        "# print(df_obs_test.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErEHgLsOhYVk"
      },
      "source": [
        "### **Load a subset of the data**\n",
        "\n",
        "We can use a subset of the dataset to train on the CNNs and Inception v2 since it is too computationaly expensive to do so.*italicised text*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAvlxZpsgnin",
        "outputId": "3b6654d5-5bb2-459e-f81a-0bad3cd4ba52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of observations for training: 100000\n",
            "Number of unique species: 500\n"
          ]
        }
      ],
      "source": [
        "n_observations = 100000\n",
        "n_species = 500\n",
        "\n",
        "path_temp = \"data\" + str(n_observations) + \"-\" + str(500) + \".csv\"\n",
        "df_obs_subset = pd.read_csv(DATA_PATH / \"data-subset\" / path_temp, sep=\",\", index_col=\"observation_id\")\n",
        "print(\"Number of observations for training: {}\".format(len(df_obs_subset)))\n",
        "y_true = df_obs_subset['species_id']\n",
        "# Relabel\n",
        "\n",
        "from sklearn import preprocessing\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(df_obs_subset.species_id)\n",
        "df_obs_subset['species_id'] = le.transform(df_obs_subset.species_id)\n",
        "\n",
        "obs_id_subset = df_obs_subset.index.values\n",
        "number_of_unique_species = np.unique(df_obs_subset['species_id']).shape[0]\n",
        "print(\"Number of unique species: \"+str(number_of_unique_species))\n",
        "X_env_train_subset = df_env.loc[obs_id_subset].values\n",
        "y_env_train_subset = y_true.values\n",
        "\n",
        "imp = SimpleImputer(\n",
        "    missing_values=np.nan,\n",
        "    strategy=\"constant\",\n",
        "    fill_value=np.finfo(np.float32).min,\n",
        ")\n",
        "imp.fit(X_env_train_subset)\n",
        "\n",
        "X_env_train_subset = imp.transform(X_env_train_subset)\n",
        "X_env_train_subset, X_env_test_subset, y_env_train_subset, y_env_test_subset = train_test_split(X_env_train_subset, y_env_train_subset, test_size=0.33, random_state=42)\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgCsTzsMierU"
      },
      "source": [
        "# Random Forest on Environmental Vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y98NKRm4iXlC",
        "outputId": "1957e5b9-0018-4265-f5d9-3c5da232e611"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-30 error rate: 57.8%\n"
          ]
        }
      ],
      "source": [
        "# print(\"Rescaling\")\n",
        "# X_train = X_train / X_train.max(axis=0)\n",
        "# X_val = X_val / X_val.max(axis=0)\n",
        "# X_test = X_test / X_test.max(axis=0)\n",
        "\n",
        "\"\"\"\n",
        "model = Sequential()\n",
        "model.add(Dense(32, activation='relu',\n",
        "                kernel_initializer='he_normal', input_shape=(n_features,)))\n",
        "model.add(Dense(16, activation='relu', kernel_initializer='he_normal'))\n",
        "model.add(Dense(1, activation='softmax'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "print(model.summary())\n",
        "model.fit(X_train, y_train, epochs=1, batch_size=32, verbose=1)\n",
        "\n",
        "\"\"\"\n",
        "filename = \"/content/gdrive/My Drive/Colab Notebooks/rf.sav\"\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=64, max_depth=10,  verbose=0, n_jobs=-1)\n",
        "\n",
        "# rf_model.fit(X_env_train_subset, y_env_train_subset)\n",
        "# pickle.dump(rf_model, open(filename, 'wb'))\n",
        "\n",
        "rf_model = pickle.load(open(filename, 'rb'))\n",
        "\n",
        "def batch_predict(predict_func, X, batch_size=1024):\n",
        "    res = predict_func(X[:1])\n",
        "    n_samples, n_outputs, dtype = X.shape[0], res.shape[1], res.dtype\n",
        "\n",
        "    preds = np.empty((n_samples, n_outputs), dtype=dtype)\n",
        "\n",
        "    for i in range(0, len(X), batch_size):\n",
        "        X_batch = X[i:i+batch_size]\n",
        "        preds[i:i+batch_size] = predict_func(X_batch)\n",
        "\n",
        "    return preds\n",
        "\n",
        "\n",
        "def predict_func(X):\n",
        "    y_score = rf_model.predict_proba(X)\n",
        "    s_pred = predict_top_30_set(y_score)\n",
        "    return s_pred\n",
        "\n",
        "\n",
        "s_val = batch_predict(predict_func, X_env_test_subset, batch_size=1024)\n",
        "score_val = top_k_error_rate_from_sets(y_env_test_subset, s_val)\n",
        "\n",
        "print(\"Top-30 error rate: {:.1%}\".format(score_val))\n",
        "\n",
        "# s_pred = batch_predict(predict_func, X_test, batch_size=1024)\n",
        "# print(\"Generate the submission file\")\n",
        "# generate_submission_file(SUBMISSION_PATH / \"random_forest_on_environmental_vectors.csv\", df_obs_test.index, s_pred)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL6TYM6ujMn2"
      },
      "source": [
        "# Deep Neural Network (Inception v2) on image patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvQvPcPZIPHJ",
        "outputId": "1dab5c09-ef34-42e7-d7c6-b8727e329dc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 256, 256, 3)       165       \n",
            "                                                                 \n",
            " inception_v3 (Functional)   (None, 2048)              21802784  \n",
            "                                                                 \n",
            " dense (Dense)               (None, 500)               1024500   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22,827,449\n",
            "Trainable params: 22,793,017\n",
            "Non-trainable params: 34,432\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "## Define model ##\n",
        "\n",
        "inception_model = Sequential()\n",
        "inception_model.add(Conv2D(3, kernel_size = 3, padding=\"same\", input_shape = (256, 256, 6), activation = 'relu'))\n",
        "inception_model.add(InceptionV3(weights='imagenet', include_top = False, input_shape= (256, 256, 3), pooling=\"avg\", classes=number_of_unique_species))\n",
        "inception_model.add(Dense(number_of_unique_species, activation = 'softmax'))\n",
        "inception_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "inception_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Jr9n5DSOcV3m"
      },
      "outputs": [],
      "source": [
        "inception_model.load_weights('/content/gdrive/My Drive/Colab Notebooks/checkpoint500-100000-16.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EX9Ovu2GMzfo"
      },
      "outputs": [],
      "source": [
        "# plotdata = []\n",
        "# y_all = np.array(pd.get_dummies(df_obs_subset['species_id']).values)\n",
        "# start = 12\n",
        "# end = 50\n",
        "# for i in range(start, end):\n",
        "#   path_temp = \"patches\" + str(n_observations) + \"-\" + str(n_species) + '-' + str(i) + \".npz\"\n",
        "#   patches = np.load(DATA_PATH  / \"data-subset\" / \"patches\" / path_temp)['arr_0']\n",
        "#   X = np.array(patches)\n",
        "#   y = y_all[i*1000:(i+1)*1000]\n",
        "#   X_train, X_test_and_val, y_train, y_test_and_val = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "#   X_test, X_val, y_test, y_val = train_test_split(X_test_and_val, y_test_and_val, test_size=0.5, random_state=42)\n",
        "\n",
        "#   print(\"\\n\\nTraining observations \" + str(i*1000+1) + \" to \" + str((i+1)*1000))\n",
        "#   inception_model.fit(X_train, y_train, epochs = 50, validation_data=(X_test, y_test), batch_size = 32, verbose=1)\n",
        "#   inception_model.save_weights('/content/gdrive/My Drive/Colab Notebooks/checkpoint500-100000-'+str(i)+'.h5')\n",
        "#   # np.savetxt('/content/gdrive/My Drive/Colab Notebooks/checkpoint'+ str(n_species) + '_'  + str(start) + '-'+str(end) +'-'+ str(i) +'.csv', np.array(plotdata), delimiter=',')\n",
        "#   (loss, accuracy) = inception_model.evaluate(x = X_test, y = y_test)\n",
        "#   print('Loss: {} Accuracy: {}'.format(loss, accuracy * 100))\n",
        "#   predictions = inception_model.predict(X_val)\n",
        "#   wrong = 0\n",
        "#   for index in range(len(y_val)):\n",
        "#       top_30_preds = predictions[index].argsort()[-30:][::-1]\n",
        "#       if le.transform(np.where(y_val[index])[0])[0] in top_30_preds:\n",
        "#         continue\n",
        "#       wrong+=1\n",
        "#   print(\"Top 30 Error Rate with \"+str((i+1)*1000)+\" observations used for training: \"+str(1.0*wrong/len(y_test)))\n",
        "#   plotdata.append([(i+1)*1000, 1.0*wrong/len(y_test)])\n",
        "\n",
        "# inception_model.save_weights('/content/gdrive/My Drive/Colab Notebooks/checkpoint500-100000.h5')\n",
        "# np.savetxt('/content/gdrive/My Drive/Colab Notebooks/plotdata_'+ str(n_species) + '_'  + str(start) + '-'+str(end) + '1.csv', np.array(plotdata), delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZrInPT4p-pA"
      },
      "source": [
        "# Ensemble Learning using RF and DNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Qy88DZBxqJCs"
      },
      "outputs": [],
      "source": [
        "# Combine the data\n",
        "\n",
        "models_list = [(rf_model, 'ev'), (inception_model, 'img')] \n",
        "\n",
        "predictions_img = np.array([])\n",
        "predictions_env = np.array([])\n",
        "y_all = np.array(pd.get_dummies(df_obs_subset['species_id']).values)\n",
        "start = 90\n",
        "end = 95\n",
        "is_first_flag = True\n",
        "X_ensemble = None\n",
        "for i in range(start, end):\n",
        "  path_temp = \"patches\" + str(n_observations) + \"-\" + str(n_species) + '-' + str(i) + \".npz\"\n",
        "  patches = np.load(DATA_PATH  / \"data-subset\" / \"patches\" / path_temp)['arr_0']\n",
        "  X_img_test = np.array(patches)\n",
        "  y_img_test = y_all[i*1000:(i+1)*1000]\n",
        "  X_img_subset_ids = np.array(df_obs_subset.index.values)[i*1000:(i+1)*1000]\n",
        "  X_env_test_subset = df_env.loc[X_img_subset_ids].values\n",
        "  X_env_test_subset = imp.transform(X_env_test_subset)\n",
        "  predictions_img = inception_model.predict(X_img_test)\n",
        "  predictions_env = rf_model.predict_proba(X_env_test_subset)\n",
        "  predictions_temp = np.hstack((predictions_img, predictions_env))\n",
        "  if (is_first_flag==True):\n",
        "    X_ensemble = predictions_temp\n",
        "    is_first_flag = False\n",
        "  else:\n",
        "    X_ensemble = np.vstack((X_ensemble, predictions_temp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nOuxH9ZCKLu",
        "outputId": "17b63da4-4b4b-4a7a-bf08-d33f62a6b9b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 30 Error Rate: 0.7727272727272727\n"
          ]
        }
      ],
      "source": [
        "# Train a random forest regressor \n",
        "y_ensemble = y_all[start*1000:end*1000]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_ensemble, y_ensemble, test_size=0.33, random_state=42)\n",
        "ensemble_model = RandomForestRegressor(n_estimators=64, max_depth=10,  verbose=0, n_jobs=-1)\n",
        "ensemble_model.fit(X_train, y_train)\n",
        "wrong = 0\n",
        "predictions = ensemble_model.predict(X_test)\n",
        "for index in range(len(y_test)):\n",
        "    top_30_preds = predictions[index].argsort()[-30:][::-1]\n",
        "    if le.transform(np.where(y_test[index])[0])[0] in top_30_preds:\n",
        "      continue\n",
        "    wrong+=1\n",
        "print(\"Top 30 Error Rate: \"+str(1.0*wrong/len(y_test)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "DL Project - GEOLife.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
